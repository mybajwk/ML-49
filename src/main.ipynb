{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA tersedia!\n",
      "Using CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch as tc\n",
    "import cupy as cp\n",
    "\n",
    "from model.ffnn import FFNN\n",
    "\n",
    "tc.autograd.set_grad_enabled(False)\n",
    "        \n",
    "if tc.cuda.is_available():\n",
    "    print(\"CUDA tersedia!\")\n",
    "else:\n",
    "    print(\"CUDA tidak tersedia!\")\n",
    "    \n",
    "\n",
    "device = tc.device(\"cuda\" if tc.cuda.is_available() else \"cpu\")\n",
    "tc.set_default_device(device)\n",
    "print(\"Using CUDA device:\", tc.cuda.get_device_name(device))\n",
    "\n",
    "\n",
    "# cp.cuda.Device(0).use()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST_784 dimuat: X shape: (70000, 784)  y shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data.astype(np.float64)\n",
    "y = mnist.target.astype(np.int32)\n",
    "\n",
    "\n",
    "X = X / 255.0\n",
    "# print(X)\n",
    "print(\"Dataset MNIST_784 dimuat: X shape:\", X.shape, \" y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data subset: Training: (56000, 784) Testing: (14000, 784)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subset_size = 70000\n",
    "indices = np.random.choice(np.arange(X.shape[0]), subset_size, replace=False)\n",
    "X_subset = X.iloc[indices]\n",
    "y_subset = y.iloc[indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"Data subset: Training:\", X_train.shape, \"Testing:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, num_classes):\n",
    "    oh = np.zeros((labels.shape[0], num_classes))\n",
    "    oh[np.arange(labels.shape[0]), labels] = 1.0\n",
    "    return oh\n",
    "\n",
    "num_classes = 10\n",
    "y_train_oh = one_hot(y_train, num_classes)\n",
    "y_test_oh = one_hot(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.40853828\n",
      "Iteration 2, loss = 0.15944046\n",
      "Iteration 3, loss = 0.11339504\n",
      "Iteration 4, loss = 0.08640177\n",
      "Iteration 5, loss = 0.06774480\n",
      "Iteration 6, loss = 0.05679317\n",
      "Iteration 7, loss = 0.04620483\n",
      "Iteration 8, loss = 0.03976023\n",
      "Iteration 9, loss = 0.03177209\n",
      "Iteration 10, loss = 0.02586417\n",
      "Iteration 11, loss = 0.02227854\n",
      "Iteration 12, loss = 0.02120353\n",
      "Iteration 13, loss = 0.01924831\n",
      "Iteration 14, loss = 0.01339245\n",
      "Iteration 15, loss = 0.01262281\n",
      "Iteration 16, loss = 0.01333962\n",
      "Iteration 17, loss = 0.01125821\n",
      "Iteration 18, loss = 0.01063192\n",
      "Iteration 19, loss = 0.01169848\n",
      "Iteration 20, loss = 0.00986919\n",
      "Iteration 21, loss = 0.00950743\n",
      "Iteration 22, loss = 0.01072083\n",
      "Iteration 23, loss = 0.00575859\n",
      "Iteration 24, loss = 0.00720443\n",
      "Iteration 25, loss = 0.00495888\n",
      "Iteration 26, loss = 0.00708670\n",
      "Iteration 27, loss = 0.00827014\n",
      "Iteration 28, loss = 0.00551774\n",
      "Iteration 29, loss = 0.00577067\n",
      "Iteration 30, loss = 0.01212517\n",
      "Iteration 31, loss = 0.00752265\n",
      "Iteration 32, loss = 0.00450850\n",
      "Iteration 33, loss = 0.00201206\n",
      "Iteration 34, loss = 0.00161348\n",
      "Iteration 35, loss = 0.00720628\n",
      "Iteration 36, loss = 0.01151789\n",
      "Iteration 37, loss = 0.00686087\n",
      "Iteration 38, loss = 0.00734413\n",
      "Iteration 39, loss = 0.00690809\n",
      "Iteration 40, loss = 0.00260483\n",
      "Iteration 41, loss = 0.00660488\n",
      "Iteration 42, loss = 0.00304541\n",
      "Iteration 43, loss = 0.00092308\n",
      "Iteration 44, loss = 0.00051853\n",
      "Iteration 45, loss = 0.00046340\n",
      "Iteration 46, loss = 0.00044752\n",
      "Iteration 47, loss = 0.00043782\n",
      "Iteration 48, loss = 0.00042988\n",
      "Iteration 49, loss = 0.00042264\n",
      "Iteration 50, loss = 0.00041633\n",
      "Iteration 51, loss = 0.00041033\n",
      "Iteration 52, loss = 0.00040465\n",
      "Iteration 53, loss = 0.00039912\n",
      "Iteration 54, loss = 0.00039370\n",
      "Iteration 55, loss = 0.00038829\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Akurasi MLPClassifier: 0.9809285714285715\n",
      "Waktu training MLPClassifier: 70.05 detik\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=100000, random_state=42, learning_rate_init=0.001, batch_size=200, verbose=True)\n",
    "\n",
    "start_time = time.time()\n",
    "mlp.fit(X_train, y_train)\n",
    "sklearn_training_time = time.time() - start_time\n",
    "\n",
    "#prediksi\n",
    "y_pred_sklearn = mlp.predict(X_test)\n",
    "accuracy_sklearn = np.mean(y_pred_sklearn == y_test)\n",
    "print(\"Akurasi MLPClassifier:\", accuracy_sklearn)\n",
    "print(\"Waktu training MLPClassifier: {:.2f} detik\".format(sklearn_training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = cp.asarray(X_train)\n",
    "# y_train_oh = cp.asarray(y_train_oh)\n",
    "# X_test = cp.asarray(X_test)\n",
    "# y_test = cp.asarray(y_test)\n",
    "\n",
    "# custom_model = FFNN(\n",
    "#     [784, 64, 10],\n",
    "#     activations_list=['relu', 'sigmoid'],\n",
    "#     loss_function='mse',  \n",
    "#     weight_init='random_uniform',\n",
    "#     init_params={'lower': -1, 'upper': 1}\n",
    "# )\n",
    "\n",
    "# epochs_custom = 10000  \n",
    "# batch_size_custom = 200\n",
    "# lr_custom = 0.001\n",
    "\n",
    "# start_time = time.time()\n",
    "# history_custom = custom_model.train(X_train, y_train_oh, epochs=epochs_custom, batch_size=batch_size_custom, learning_rate=lr_custom, verbose=False)\n",
    "# custom_training_time = time.time() - start_time\n",
    "# print(f\"Model custom selesai training dalam {custom_training_time:.2f} detik.\")\n",
    "\n",
    "# # Prediksi pada data test\n",
    "# y_pred_custom = custom_model.forward(X_test)\n",
    "# # y_pred_custom.data adalah NumPy array, sehingga gunakan np.argmax dengan parameter axis\n",
    "# y_pred_custom_labels = cp.argmax(y_pred_custom, axis=1)\n",
    "# accuracy_custom = cp.mean(y_pred_custom_labels == y_test)\n",
    "# print(\"Akurasi model custom FFNN:\", accuracy_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/100000 - Train Loss: 2.29328291\n",
      "Epoch 2/100000 - Train Loss: 2.06913726\n",
      "Epoch 3/100000 - Train Loss: 1.84946939\n",
      "Epoch 4/100000 - Train Loss: 1.61635886\n",
      "Epoch 5/100000 - Train Loss: 1.38599320\n",
      "Epoch 6/100000 - Train Loss: 1.18285971\n",
      "Epoch 7/100000 - Train Loss: 1.01915672\n",
      "Epoch 8/100000 - Train Loss: 0.89366738\n",
      "Epoch 9/100000 - Train Loss: 0.79865181\n",
      "Epoch 10/100000 - Train Loss: 0.72561795\n",
      "Epoch 11/100000 - Train Loss: 0.66815051\n",
      "Epoch 12/100000 - Train Loss: 0.62185101\n",
      "Epoch 13/100000 - Train Loss: 0.58380330\n",
      "Epoch 14/100000 - Train Loss: 0.55203352\n",
      "Epoch 15/100000 - Train Loss: 0.52524581\n",
      "Epoch 16/100000 - Train Loss: 0.50233176\n",
      "Epoch 17/100000 - Train Loss: 0.48255585\n",
      "Epoch 18/100000 - Train Loss: 0.46530987\n",
      "Epoch 19/100000 - Train Loss: 0.45012938\n",
      "Epoch 20/100000 - Train Loss: 0.43671998\n",
      "Epoch 21/100000 - Train Loss: 0.42469192\n",
      "Epoch 22/100000 - Train Loss: 0.41395158\n",
      "Epoch 23/100000 - Train Loss: 0.40426377\n",
      "Epoch 24/100000 - Train Loss: 0.39538600\n",
      "Epoch 25/100000 - Train Loss: 0.38732596\n",
      "Epoch 26/100000 - Train Loss: 0.37997101\n",
      "Epoch 27/100000 - Train Loss: 0.37311626\n",
      "Epoch 28/100000 - Train Loss: 0.36682729\n",
      "Epoch 29/100000 - Train Loss: 0.36088102\n",
      "Epoch 30/100000 - Train Loss: 0.35546339\n",
      "Epoch 31/100000 - Train Loss: 0.35029853\n",
      "Epoch 32/100000 - Train Loss: 0.34551013\n",
      "Epoch 33/100000 - Train Loss: 0.34093050\n",
      "Epoch 34/100000 - Train Loss: 0.33667106\n",
      "Epoch 35/100000 - Train Loss: 0.33258448\n",
      "Epoch 36/100000 - Train Loss: 0.32877391\n",
      "Epoch 37/100000 - Train Loss: 0.32502770\n",
      "Epoch 38/100000 - Train Loss: 0.32156296\n",
      "Epoch 39/100000 - Train Loss: 0.31817261\n",
      "Epoch 40/100000 - Train Loss: 0.31499595\n",
      "Epoch 41/100000 - Train Loss: 0.31190439\n",
      "Epoch 42/100000 - Train Loss: 0.30892574\n",
      "Epoch 43/100000 - Train Loss: 0.30610031\n",
      "Epoch 44/100000 - Train Loss: 0.30331603\n",
      "Epoch 45/100000 - Train Loss: 0.30073201\n",
      "Epoch 46/100000 - Train Loss: 0.29807588\n",
      "Epoch 47/100000 - Train Loss: 0.29566786\n",
      "Epoch 48/100000 - Train Loss: 0.29322396\n",
      "Epoch 49/100000 - Train Loss: 0.29094996\n",
      "Epoch 50/100000 - Train Loss: 0.28865397\n",
      "Epoch 51/100000 - Train Loss: 0.28647608\n",
      "Epoch 52/100000 - Train Loss: 0.28433680\n",
      "Epoch 53/100000 - Train Loss: 0.28227569\n",
      "Epoch 54/100000 - Train Loss: 0.28025158\n",
      "Epoch 55/100000 - Train Loss: 0.27826368\n",
      "Epoch 56/100000 - Train Loss: 0.27637817\n",
      "Epoch 57/100000 - Train Loss: 0.27444697\n",
      "Epoch 58/100000 - Train Loss: 0.27264425\n",
      "Epoch 59/100000 - Train Loss: 0.27082129\n",
      "Epoch 60/100000 - Train Loss: 0.26903925\n",
      "Epoch 61/100000 - Train Loss: 0.26731600\n",
      "Epoch 62/100000 - Train Loss: 0.26566821\n",
      "Epoch 63/100000 - Train Loss: 0.26399837\n",
      "Epoch 64/100000 - Train Loss: 0.26238697\n",
      "Epoch 65/100000 - Train Loss: 0.26076381\n",
      "Epoch 66/100000 - Train Loss: 0.25921818\n",
      "Epoch 67/100000 - Train Loss: 0.25765872\n",
      "Epoch 68/100000 - Train Loss: 0.25621023\n",
      "Epoch 69/100000 - Train Loss: 0.25463445\n",
      "Epoch 70/100000 - Train Loss: 0.25320770\n",
      "Epoch 71/100000 - Train Loss: 0.25181693\n",
      "Epoch 72/100000 - Train Loss: 0.25037030\n",
      "Epoch 73/100000 - Train Loss: 0.24900234\n",
      "Epoch 74/100000 - Train Loss: 0.24753743\n",
      "Epoch 75/100000 - Train Loss: 0.24620088\n",
      "Epoch 76/100000 - Train Loss: 0.24486827\n",
      "Epoch 77/100000 - Train Loss: 0.24361851\n",
      "Epoch 78/100000 - Train Loss: 0.24231028\n",
      "Epoch 79/100000 - Train Loss: 0.24110857\n",
      "Epoch 80/100000 - Train Loss: 0.23979680\n",
      "Epoch 81/100000 - Train Loss: 0.23857888\n",
      "Epoch 82/100000 - Train Loss: 0.23732478\n",
      "Epoch 83/100000 - Train Loss: 0.23613500\n",
      "Epoch 84/100000 - Train Loss: 0.23494824\n",
      "Epoch 85/100000 - Train Loss: 0.23373975\n",
      "Epoch 86/100000 - Train Loss: 0.23262062\n",
      "Epoch 87/100000 - Train Loss: 0.23147114\n",
      "Epoch 88/100000 - Train Loss: 0.23036545\n",
      "Epoch 89/100000 - Train Loss: 0.22927923\n",
      "Epoch 90/100000 - Train Loss: 0.22815356\n",
      "Epoch 91/100000 - Train Loss: 0.22703509\n",
      "Epoch 92/100000 - Train Loss: 0.22593574\n",
      "Epoch 93/100000 - Train Loss: 0.22490918\n",
      "Epoch 94/100000 - Train Loss: 0.22383262\n",
      "Epoch 95/100000 - Train Loss: 0.22279661\n",
      "Epoch 96/100000 - Train Loss: 0.22175661\n",
      "Epoch 97/100000 - Train Loss: 0.22072967\n",
      "Epoch 98/100000 - Train Loss: 0.21970802\n",
      "Epoch 99/100000 - Train Loss: 0.21872351\n",
      "Epoch 100/100000 - Train Loss: 0.21774349\n",
      "Epoch 101/100000 - Train Loss: 0.21674300\n",
      "Epoch 102/100000 - Train Loss: 0.21574614\n",
      "Epoch 103/100000 - Train Loss: 0.21471501\n",
      "Epoch 104/100000 - Train Loss: 0.21382333\n",
      "Epoch 105/100000 - Train Loss: 0.21286687\n",
      "Epoch 106/100000 - Train Loss: 0.21190251\n",
      "Epoch 107/100000 - Train Loss: 0.21101361\n",
      "Epoch 108/100000 - Train Loss: 0.21006376\n",
      "Epoch 109/100000 - Train Loss: 0.20913527\n",
      "Epoch 110/100000 - Train Loss: 0.20819850\n",
      "Epoch 111/100000 - Train Loss: 0.20732941\n",
      "Epoch 112/100000 - Train Loss: 0.20643081\n",
      "Epoch 113/100000 - Train Loss: 0.20553469\n",
      "Epoch 114/100000 - Train Loss: 0.20469614\n",
      "Epoch 115/100000 - Train Loss: 0.20380471\n",
      "Epoch 116/100000 - Train Loss: 0.20295267\n",
      "Epoch 117/100000 - Train Loss: 0.20207141\n",
      "Epoch 118/100000 - Train Loss: 0.20128453\n",
      "Epoch 119/100000 - Train Loss: 0.20042318\n",
      "Epoch 120/100000 - Train Loss: 0.19956964\n",
      "Epoch 121/100000 - Train Loss: 0.19880167\n",
      "Epoch 122/100000 - Train Loss: 0.19795833\n",
      "Epoch 123/100000 - Train Loss: 0.19721768\n",
      "Epoch 124/100000 - Train Loss: 0.19635388\n",
      "Epoch 125/100000 - Train Loss: 0.19557266\n",
      "Epoch 126/100000 - Train Loss: 0.19478461\n",
      "Epoch 127/100000 - Train Loss: 0.19398670\n",
      "Epoch 128/100000 - Train Loss: 0.19318012\n",
      "Epoch 129/100000 - Train Loss: 0.19250145\n",
      "Epoch 130/100000 - Train Loss: 0.19165133\n",
      "Epoch 131/100000 - Train Loss: 0.19094047\n",
      "Epoch 132/100000 - Train Loss: 0.19019445\n",
      "Epoch 133/100000 - Train Loss: 0.18942557\n",
      "Epoch 134/100000 - Train Loss: 0.18864185\n",
      "Epoch 135/100000 - Train Loss: 0.18795520\n",
      "Epoch 136/100000 - Train Loss: 0.18723197\n",
      "Epoch 137/100000 - Train Loss: 0.18649584\n",
      "Epoch 138/100000 - Train Loss: 0.18572898\n",
      "Epoch 139/100000 - Train Loss: 0.18505992\n",
      "Epoch 140/100000 - Train Loss: 0.18429828\n",
      "Epoch 141/100000 - Train Loss: 0.18360783\n",
      "Epoch 142/100000 - Train Loss: 0.18295175\n",
      "Epoch 143/100000 - Train Loss: 0.18229188\n",
      "Epoch 144/100000 - Train Loss: 0.18157058\n",
      "Epoch 145/100000 - Train Loss: 0.18083976\n",
      "Epoch 146/100000 - Train Loss: 0.18024014\n",
      "Epoch 147/100000 - Train Loss: 0.17954847\n",
      "Epoch 148/100000 - Train Loss: 0.17889737\n",
      "Epoch 149/100000 - Train Loss: 0.17822174\n",
      "Epoch 150/100000 - Train Loss: 0.17753844\n",
      "Epoch 151/100000 - Train Loss: 0.17690914\n",
      "Epoch 152/100000 - Train Loss: 0.17629065\n",
      "Epoch 153/100000 - Train Loss: 0.17561291\n",
      "Epoch 154/100000 - Train Loss: 0.17496059\n",
      "Epoch 155/100000 - Train Loss: 0.17437718\n",
      "Epoch 156/100000 - Train Loss: 0.17371023\n",
      "Epoch 157/100000 - Train Loss: 0.17303728\n",
      "Epoch 158/100000 - Train Loss: 0.17241543\n",
      "Epoch 159/100000 - Train Loss: 0.17180766\n",
      "Epoch 160/100000 - Train Loss: 0.17121266\n",
      "Epoch 161/100000 - Train Loss: 0.17061447\n",
      "Epoch 162/100000 - Train Loss: 0.16997652\n",
      "Epoch 163/100000 - Train Loss: 0.16936466\n",
      "Epoch 164/100000 - Train Loss: 0.16876108\n",
      "Epoch 165/100000 - Train Loss: 0.16819494\n",
      "Epoch 166/100000 - Train Loss: 0.16758540\n",
      "Epoch 167/100000 - Train Loss: 0.16699011\n",
      "Epoch 168/100000 - Train Loss: 0.16639162\n",
      "Epoch 169/100000 - Train Loss: 0.16582326\n",
      "Epoch 170/100000 - Train Loss: 0.16522991\n",
      "Epoch 171/100000 - Train Loss: 0.16465579\n",
      "Epoch 172/100000 - Train Loss: 0.16407240\n",
      "Epoch 173/100000 - Train Loss: 0.16348443\n",
      "Epoch 174/100000 - Train Loss: 0.16293671\n",
      "Epoch 175/100000 - Train Loss: 0.16237820\n",
      "Epoch 176/100000 - Train Loss: 0.16175315\n",
      "Epoch 177/100000 - Train Loss: 0.16123363\n",
      "Epoch 178/100000 - Train Loss: 0.16068613\n",
      "Epoch 179/100000 - Train Loss: 0.16017791\n",
      "Epoch 180/100000 - Train Loss: 0.15959626\n",
      "Epoch 181/100000 - Train Loss: 0.15905484\n",
      "Epoch 182/100000 - Train Loss: 0.15851884\n",
      "Epoch 183/100000 - Train Loss: 0.15794900\n",
      "Epoch 184/100000 - Train Loss: 0.15743812\n",
      "Epoch 185/100000 - Train Loss: 0.15688628\n",
      "Epoch 186/100000 - Train Loss: 0.15632724\n",
      "Epoch 187/100000 - Train Loss: 0.15583061\n",
      "Epoch 188/100000 - Train Loss: 0.15536234\n",
      "Epoch 189/100000 - Train Loss: 0.15481738\n",
      "Epoch 190/100000 - Train Loss: 0.15429679\n",
      "Epoch 191/100000 - Train Loss: 0.15377104\n",
      "Epoch 192/100000 - Train Loss: 0.15325728\n",
      "Epoch 193/100000 - Train Loss: 0.15279023\n",
      "Epoch 194/100000 - Train Loss: 0.15225630\n",
      "Epoch 195/100000 - Train Loss: 0.15173208\n",
      "Epoch 196/100000 - Train Loss: 0.15126003\n",
      "Epoch 197/100000 - Train Loss: 0.15074223\n",
      "Epoch 198/100000 - Train Loss: 0.15027126\n",
      "Epoch 199/100000 - Train Loss: 0.14976848\n",
      "Epoch 200/100000 - Train Loss: 0.14928657\n",
      "Epoch 201/100000 - Train Loss: 0.14879072\n",
      "Epoch 202/100000 - Train Loss: 0.14833144\n",
      "Epoch 203/100000 - Train Loss: 0.14782841\n",
      "Epoch 204/100000 - Train Loss: 0.14734988\n",
      "Epoch 205/100000 - Train Loss: 0.14686250\n",
      "Epoch 206/100000 - Train Loss: 0.14643329\n",
      "Epoch 207/100000 - Train Loss: 0.14596275\n",
      "Epoch 208/100000 - Train Loss: 0.14549550\n",
      "Epoch 209/100000 - Train Loss: 0.14500612\n",
      "Epoch 210/100000 - Train Loss: 0.14453935\n",
      "Epoch 211/100000 - Train Loss: 0.14410254\n",
      "Epoch 212/100000 - Train Loss: 0.14361952\n",
      "Epoch 213/100000 - Train Loss: 0.14320404\n",
      "Epoch 214/100000 - Train Loss: 0.14273265\n",
      "Epoch 215/100000 - Train Loss: 0.14231708\n",
      "Epoch 216/100000 - Train Loss: 0.14185968\n",
      "Epoch 217/100000 - Train Loss: 0.14137904\n",
      "Epoch 218/100000 - Train Loss: 0.14096800\n",
      "Epoch 219/100000 - Train Loss: 0.14051456\n",
      "Epoch 220/100000 - Train Loss: 0.14004797\n",
      "Epoch 221/100000 - Train Loss: 0.13967167\n",
      "Epoch 222/100000 - Train Loss: 0.13920760\n",
      "Epoch 223/100000 - Train Loss: 0.13881158\n",
      "Epoch 224/100000 - Train Loss: 0.13834173\n",
      "Epoch 225/100000 - Train Loss: 0.13796668\n",
      "Epoch 226/100000 - Train Loss: 0.13751922\n",
      "Epoch 227/100000 - Train Loss: 0.13714181\n",
      "Epoch 228/100000 - Train Loss: 0.13666630\n",
      "Epoch 229/100000 - Train Loss: 0.13627414\n",
      "Epoch 230/100000 - Train Loss: 0.13586721\n",
      "Epoch 231/100000 - Train Loss: 0.13543818\n",
      "Epoch 232/100000 - Train Loss: 0.13500477\n",
      "Epoch 233/100000 - Train Loss: 0.13462702\n",
      "Epoch 234/100000 - Train Loss: 0.13425130\n",
      "Epoch 235/100000 - Train Loss: 0.13381239\n",
      "Epoch 236/100000 - Train Loss: 0.13342469\n",
      "Epoch 237/100000 - Train Loss: 0.13300526\n",
      "Epoch 238/100000 - Train Loss: 0.13260648\n",
      "Epoch 239/100000 - Train Loss: 0.13221703\n",
      "Epoch 240/100000 - Train Loss: 0.13184565\n",
      "Epoch 241/100000 - Train Loss: 0.13143564\n",
      "Epoch 242/100000 - Train Loss: 0.13107580\n",
      "Epoch 243/100000 - Train Loss: 0.13066313\n",
      "Epoch 244/100000 - Train Loss: 0.13030942\n",
      "Epoch 245/100000 - Train Loss: 0.12987514\n",
      "Epoch 246/100000 - Train Loss: 0.12952669\n",
      "Epoch 247/100000 - Train Loss: 0.12915112\n",
      "Epoch 248/100000 - Train Loss: 0.12872858\n",
      "Epoch 249/100000 - Train Loss: 0.12836536\n",
      "Epoch 250/100000 - Train Loss: 0.12798326\n",
      "Epoch 251/100000 - Train Loss: 0.12764647\n",
      "Epoch 252/100000 - Train Loss: 0.12727263\n",
      "Epoch 253/100000 - Train Loss: 0.12689287\n",
      "Epoch 254/100000 - Train Loss: 0.12649353\n",
      "Epoch 255/100000 - Train Loss: 0.12615476\n",
      "Epoch 256/100000 - Train Loss: 0.12579257\n",
      "Epoch 257/100000 - Train Loss: 0.12543181\n",
      "Epoch 258/100000 - Train Loss: 0.12504902\n",
      "Epoch 259/100000 - Train Loss: 0.12473007\n",
      "Epoch 260/100000 - Train Loss: 0.12431424\n",
      "Epoch 261/100000 - Train Loss: 0.12400903\n",
      "Epoch 262/100000 - Train Loss: 0.12362049\n",
      "Epoch 263/100000 - Train Loss: 0.12328756\n",
      "Epoch 264/100000 - Train Loss: 0.12293035\n",
      "Epoch 265/100000 - Train Loss: 0.12260711\n",
      "Epoch 266/100000 - Train Loss: 0.12227139\n",
      "Epoch 267/100000 - Train Loss: 0.12186282\n",
      "Epoch 268/100000 - Train Loss: 0.12160076\n",
      "Epoch 269/100000 - Train Loss: 0.12122057\n",
      "Epoch 270/100000 - Train Loss: 0.12083773\n",
      "Epoch 271/100000 - Train Loss: 0.12051940\n",
      "Epoch 272/100000 - Train Loss: 0.12018936\n",
      "Epoch 273/100000 - Train Loss: 0.11988527\n",
      "Epoch 274/100000 - Train Loss: 0.11952528\n",
      "Epoch 275/100000 - Train Loss: 0.11919532\n",
      "Epoch 276/100000 - Train Loss: 0.11883149\n",
      "Epoch 277/100000 - Train Loss: 0.11856134\n",
      "Epoch 278/100000 - Train Loss: 0.11818187\n",
      "Epoch 279/100000 - Train Loss: 0.11790895\n",
      "Epoch 280/100000 - Train Loss: 0.11754084\n",
      "Epoch 281/100000 - Train Loss: 0.11723543\n",
      "Epoch 282/100000 - Train Loss: 0.11685455\n",
      "Epoch 283/100000 - Train Loss: 0.11651075\n",
      "Epoch 284/100000 - Train Loss: 0.11629745\n",
      "Epoch 285/100000 - Train Loss: 0.11591100\n",
      "Epoch 286/100000 - Train Loss: 0.11562058\n",
      "Epoch 287/100000 - Train Loss: 0.11531248\n",
      "Epoch 288/100000 - Train Loss: 0.11502200\n",
      "Epoch 289/100000 - Train Loss: 0.11468702\n",
      "Epoch 290/100000 - Train Loss: 0.11436462\n",
      "Epoch 291/100000 - Train Loss: 0.11406676\n",
      "Epoch 292/100000 - Train Loss: 0.11375842\n",
      "Epoch 293/100000 - Train Loss: 0.11346892\n",
      "Epoch 294/100000 - Train Loss: 0.11315357\n",
      "Epoch 295/100000 - Train Loss: 0.11284384\n",
      "Epoch 296/100000 - Train Loss: 0.11255088\n",
      "Epoch 297/100000 - Train Loss: 0.11225866\n",
      "Epoch 298/100000 - Train Loss: 0.11196042\n",
      "Epoch 299/100000 - Train Loss: 0.11163353\n",
      "Epoch 300/100000 - Train Loss: 0.11136304\n",
      "Epoch 301/100000 - Train Loss: 0.11104748\n",
      "Epoch 302/100000 - Train Loss: 0.11077598\n",
      "Epoch 303/100000 - Train Loss: 0.11044958\n",
      "Epoch 304/100000 - Train Loss: 0.11016467\n",
      "Epoch 305/100000 - Train Loss: 0.10987616\n",
      "Epoch 306/100000 - Train Loss: 0.10960171\n",
      "Epoch 307/100000 - Train Loss: 0.10929578\n",
      "Epoch 308/100000 - Train Loss: 0.10901694\n",
      "Epoch 309/100000 - Train Loss: 0.10872920\n",
      "Epoch 310/100000 - Train Loss: 0.10844727\n",
      "Epoch 311/100000 - Train Loss: 0.10819651\n",
      "Epoch 312/100000 - Train Loss: 0.10789809\n",
      "Epoch 313/100000 - Train Loss: 0.10761488\n",
      "Epoch 314/100000 - Train Loss: 0.10732248\n",
      "Epoch 315/100000 - Train Loss: 0.10704175\n",
      "Epoch 316/100000 - Train Loss: 0.10673166\n",
      "Epoch 317/100000 - Train Loss: 0.10649645\n",
      "Epoch 318/100000 - Train Loss: 0.10623699\n",
      "Epoch 319/100000 - Train Loss: 0.10595395\n",
      "Epoch 320/100000 - Train Loss: 0.10568686\n",
      "Epoch 321/100000 - Train Loss: 0.10541859\n",
      "Epoch 322/100000 - Train Loss: 0.10513420\n",
      "Epoch 323/100000 - Train Loss: 0.10484919\n",
      "Epoch 324/100000 - Train Loss: 0.10458219\n",
      "Epoch 325/100000 - Train Loss: 0.10433357\n",
      "Epoch 326/100000 - Train Loss: 0.10404747\n",
      "Epoch 327/100000 - Train Loss: 0.10381863\n",
      "Epoch 328/100000 - Train Loss: 0.10353670\n",
      "Epoch 329/100000 - Train Loss: 0.10324685\n",
      "Epoch 330/100000 - Train Loss: 0.10298769\n",
      "Epoch 331/100000 - Train Loss: 0.10277868\n",
      "Epoch 332/100000 - Train Loss: 0.10247581\n",
      "Epoch 333/100000 - Train Loss: 0.10223989\n",
      "Epoch 334/100000 - Train Loss: 0.10196693\n",
      "Epoch 335/100000 - Train Loss: 0.10171569\n",
      "Epoch 336/100000 - Train Loss: 0.10145628\n",
      "Epoch 337/100000 - Train Loss: 0.10116080\n",
      "Epoch 338/100000 - Train Loss: 0.10097917\n",
      "Epoch 339/100000 - Train Loss: 0.10069447\n",
      "Epoch 340/100000 - Train Loss: 0.10041988\n",
      "Epoch 341/100000 - Train Loss: 0.10019664\n",
      "Epoch 342/100000 - Train Loss: 0.09989679\n",
      "Epoch 343/100000 - Train Loss: 0.09967236\n",
      "Epoch 344/100000 - Train Loss: 0.09940496\n",
      "Epoch 345/100000 - Train Loss: 0.09919007\n",
      "Epoch 346/100000 - Train Loss: 0.09892022\n",
      "Epoch 347/100000 - Train Loss: 0.09867799\n",
      "Epoch 348/100000 - Train Loss: 0.09844841\n",
      "Epoch 349/100000 - Train Loss: 0.09818687\n",
      "Epoch 350/100000 - Train Loss: 0.09794438\n",
      "Epoch 351/100000 - Train Loss: 0.09771989\n",
      "Epoch 352/100000 - Train Loss: 0.09747698\n",
      "Epoch 353/100000 - Train Loss: 0.09721657\n",
      "Epoch 354/100000 - Train Loss: 0.09699987\n",
      "Epoch 355/100000 - Train Loss: 0.09675745\n",
      "Epoch 356/100000 - Train Loss: 0.09654184\n",
      "Epoch 357/100000 - Train Loss: 0.09628152\n",
      "Epoch 358/100000 - Train Loss: 0.09602752\n",
      "Epoch 359/100000 - Train Loss: 0.09580906\n",
      "Epoch 360/100000 - Train Loss: 0.09559927\n",
      "Epoch 361/100000 - Train Loss: 0.09535053\n",
      "Epoch 362/100000 - Train Loss: 0.09511598\n",
      "Epoch 363/100000 - Train Loss: 0.09487189\n",
      "Epoch 364/100000 - Train Loss: 0.09463631\n",
      "Epoch 365/100000 - Train Loss: 0.09443461\n",
      "Epoch 366/100000 - Train Loss: 0.09419931\n",
      "Epoch 367/100000 - Train Loss: 0.09397790\n",
      "Epoch 368/100000 - Train Loss: 0.09374610\n",
      "Epoch 369/100000 - Train Loss: 0.09349380\n",
      "Epoch 370/100000 - Train Loss: 0.09330204\n",
      "Epoch 371/100000 - Train Loss: 0.09301728\n",
      "Epoch 372/100000 - Train Loss: 0.09283996\n",
      "Epoch 373/100000 - Train Loss: 0.09260282\n",
      "Epoch 374/100000 - Train Loss: 0.09242141\n",
      "Epoch 375/100000 - Train Loss: 0.09217914\n",
      "Epoch 376/100000 - Train Loss: 0.09197098\n",
      "Epoch 377/100000 - Train Loss: 0.09171802\n",
      "Epoch 378/100000 - Train Loss: 0.09150255\n",
      "Epoch 379/100000 - Train Loss: 0.09129804\n",
      "Epoch 380/100000 - Train Loss: 0.09108165\n",
      "Epoch 381/100000 - Train Loss: 0.09085649\n",
      "Epoch 382/100000 - Train Loss: 0.09062449\n",
      "Epoch 383/100000 - Train Loss: 0.09038295\n",
      "Epoch 384/100000 - Train Loss: 0.09023720\n",
      "Epoch 385/100000 - Train Loss: 0.09001080\n",
      "Epoch 386/100000 - Train Loss: 0.08977597\n",
      "Epoch 387/100000 - Train Loss: 0.08958971\n",
      "Epoch 388/100000 - Train Loss: 0.08937696\n",
      "Epoch 389/100000 - Train Loss: 0.08914943\n",
      "Epoch 390/100000 - Train Loss: 0.08894357\n",
      "Epoch 391/100000 - Train Loss: 0.08870734\n",
      "Epoch 392/100000 - Train Loss: 0.08854804\n",
      "Epoch 393/100000 - Train Loss: 0.08829678\n",
      "Epoch 394/100000 - Train Loss: 0.08814146\n",
      "Epoch 395/100000 - Train Loss: 0.08786913\n",
      "Epoch 396/100000 - Train Loss: 0.08769899\n",
      "Epoch 397/100000 - Train Loss: 0.08750398\n",
      "Epoch 398/100000 - Train Loss: 0.08726713\n",
      "Epoch 399/100000 - Train Loss: 0.08706434\n",
      "Epoch 400/100000 - Train Loss: 0.08687854\n",
      "Epoch 401/100000 - Train Loss: 0.08664480\n",
      "Epoch 402/100000 - Train Loss: 0.08647335\n",
      "Epoch 403/100000 - Train Loss: 0.08627721\n",
      "Epoch 404/100000 - Train Loss: 0.08608165\n",
      "Epoch 405/100000 - Train Loss: 0.08587992\n",
      "Epoch 406/100000 - Train Loss: 0.08567893\n",
      "Epoch 407/100000 - Train Loss: 0.08543947\n",
      "Epoch 408/100000 - Train Loss: 0.08527030\n",
      "Epoch 409/100000 - Train Loss: 0.08505276\n",
      "Epoch 410/100000 - Train Loss: 0.08489370\n",
      "Epoch 411/100000 - Train Loss: 0.08467681\n",
      "Epoch 412/100000 - Train Loss: 0.08449144\n",
      "Epoch 413/100000 - Train Loss: 0.08430896\n",
      "Epoch 414/100000 - Train Loss: 0.08411968\n",
      "Epoch 415/100000 - Train Loss: 0.08391526\n",
      "Epoch 416/100000 - Train Loss: 0.08370121\n",
      "Epoch 417/100000 - Train Loss: 0.08352161\n",
      "Epoch 418/100000 - Train Loss: 0.08332513\n",
      "Epoch 419/100000 - Train Loss: 0.08311250\n",
      "Epoch 420/100000 - Train Loss: 0.08294705\n",
      "Epoch 421/100000 - Train Loss: 0.08271938\n",
      "Epoch 422/100000 - Train Loss: 0.08259060\n",
      "Epoch 423/100000 - Train Loss: 0.08238778\n",
      "Epoch 424/100000 - Train Loss: 0.08219720\n",
      "Epoch 425/100000 - Train Loss: 0.08200458\n",
      "Epoch 426/100000 - Train Loss: 0.08181406\n",
      "Epoch 427/100000 - Train Loss: 0.08163595\n",
      "Epoch 428/100000 - Train Loss: 0.08144877\n",
      "Epoch 429/100000 - Train Loss: 0.08129755\n",
      "Epoch 430/100000 - Train Loss: 0.08107340\n",
      "Epoch 431/100000 - Train Loss: 0.08091100\n",
      "Epoch 432/100000 - Train Loss: 0.08067483\n",
      "Epoch 433/100000 - Train Loss: 0.08053144\n",
      "Epoch 434/100000 - Train Loss: 0.08036350\n",
      "Epoch 435/100000 - Train Loss: 0.08013820\n",
      "Epoch 436/100000 - Train Loss: 0.08000069\n",
      "Epoch 437/100000 - Train Loss: 0.07980648\n",
      "Epoch 438/100000 - Train Loss: 0.07962451\n",
      "Epoch 439/100000 - Train Loss: 0.07945153\n",
      "Epoch 440/100000 - Train Loss: 0.07927295\n",
      "Epoch 441/100000 - Train Loss: 0.07908336\n",
      "Epoch 442/100000 - Train Loss: 0.07891653\n",
      "Epoch 443/100000 - Train Loss: 0.07875763\n",
      "Epoch 444/100000 - Train Loss: 0.07855713\n",
      "Epoch 445/100000 - Train Loss: 0.07841372\n",
      "Epoch 446/100000 - Train Loss: 0.07823197\n",
      "Epoch 447/100000 - Train Loss: 0.07797628\n",
      "Epoch 448/100000 - Train Loss: 0.07789631\n",
      "Epoch 449/100000 - Train Loss: 0.07771096\n",
      "Epoch 450/100000 - Train Loss: 0.07752076\n",
      "Epoch 451/100000 - Train Loss: 0.07734747\n",
      "Epoch 452/100000 - Train Loss: 0.07720901\n",
      "Epoch 453/100000 - Train Loss: 0.07704162\n",
      "Epoch 454/100000 - Train Loss: 0.07684802\n",
      "Epoch 455/100000 - Train Loss: 0.07668895\n",
      "Epoch 456/100000 - Train Loss: 0.07653538\n",
      "Epoch 457/100000 - Train Loss: 0.07636185\n",
      "Epoch 458/100000 - Train Loss: 0.07617206\n",
      "Epoch 459/100000 - Train Loss: 0.07600880\n",
      "Epoch 460/100000 - Train Loss: 0.07584249\n",
      "Epoch 461/100000 - Train Loss: 0.07567671\n",
      "Epoch 462/100000 - Train Loss: 0.07551576\n",
      "Epoch 463/100000 - Train Loss: 0.07536127\n",
      "Epoch 464/100000 - Train Loss: 0.07520533\n",
      "Epoch 465/100000 - Train Loss: 0.07505189\n",
      "Epoch 466/100000 - Train Loss: 0.07486288\n",
      "Epoch 467/100000 - Train Loss: 0.07468564\n",
      "Epoch 468/100000 - Train Loss: 0.07454006\n",
      "Epoch 469/100000 - Train Loss: 0.07438207\n",
      "Epoch 470/100000 - Train Loss: 0.07420859\n",
      "Epoch 471/100000 - Train Loss: 0.07405402\n",
      "Epoch 472/100000 - Train Loss: 0.07390966\n",
      "Epoch 473/100000 - Train Loss: 0.07372042\n",
      "Epoch 474/100000 - Train Loss: 0.07356606\n",
      "Epoch 475/100000 - Train Loss: 0.07336049\n",
      "Epoch 476/100000 - Train Loss: 0.07324669\n",
      "Epoch 477/100000 - Train Loss: 0.07309893\n",
      "Epoch 478/100000 - Train Loss: 0.07293958\n",
      "Epoch 479/100000 - Train Loss: 0.07274439\n",
      "Epoch 480/100000 - Train Loss: 0.07259988\n",
      "Epoch 481/100000 - Train Loss: 0.07246851\n",
      "Epoch 482/100000 - Train Loss: 0.07232238\n",
      "Epoch 483/100000 - Train Loss: 0.07216575\n",
      "Epoch 484/100000 - Train Loss: 0.07199256\n",
      "Epoch 485/100000 - Train Loss: 0.07186241\n",
      "Epoch 486/100000 - Train Loss: 0.07170283\n",
      "Epoch 487/100000 - Train Loss: 0.07157440\n",
      "Epoch 488/100000 - Train Loss: 0.07137145\n",
      "Epoch 489/100000 - Train Loss: 0.07124597\n",
      "Epoch 490/100000 - Train Loss: 0.07106239\n",
      "Epoch 491/100000 - Train Loss: 0.07091036\n",
      "Epoch 492/100000 - Train Loss: 0.07077148\n",
      "Epoch 493/100000 - Train Loss: 0.07063910\n",
      "Epoch 494/100000 - Train Loss: 0.07049445\n",
      "Epoch 495/100000 - Train Loss: 0.07032784\n",
      "Epoch 496/100000 - Train Loss: 0.07018842\n",
      "Epoch 497/100000 - Train Loss: 0.07005379\n",
      "Epoch 498/100000 - Train Loss: 0.06989674\n",
      "Epoch 499/100000 - Train Loss: 0.06974766\n",
      "Epoch 500/100000 - Train Loss: 0.06957682\n",
      "Epoch 501/100000 - Train Loss: 0.06945403\n",
      "Epoch 502/100000 - Train Loss: 0.06931925\n",
      "Epoch 503/100000 - Train Loss: 0.06915749\n",
      "Epoch 504/100000 - Train Loss: 0.06901488\n",
      "Epoch 505/100000 - Train Loss: 0.06886337\n",
      "Epoch 506/100000 - Train Loss: 0.06873676\n",
      "Epoch 507/100000 - Train Loss: 0.06858617\n",
      "Epoch 508/100000 - Train Loss: 0.06843563\n",
      "Epoch 509/100000 - Train Loss: 0.06830024\n",
      "Epoch 510/100000 - Train Loss: 0.06814334\n",
      "Epoch 511/100000 - Train Loss: 0.06798174\n",
      "Epoch 512/100000 - Train Loss: 0.06784350\n",
      "Epoch 513/100000 - Train Loss: 0.06773566\n",
      "Epoch 514/100000 - Train Loss: 0.06759975\n",
      "Epoch 515/100000 - Train Loss: 0.06740274\n",
      "Epoch 516/100000 - Train Loss: 0.06728390\n",
      "Epoch 517/100000 - Train Loss: 0.06715796\n",
      "Epoch 518/100000 - Train Loss: 0.06697800\n",
      "Epoch 519/100000 - Train Loss: 0.06689648\n",
      "Epoch 520/100000 - Train Loss: 0.06669332\n",
      "Epoch 521/100000 - Train Loss: 0.06662803\n",
      "Epoch 522/100000 - Train Loss: 0.06646427\n",
      "Epoch 523/100000 - Train Loss: 0.06632597\n",
      "Epoch 524/100000 - Train Loss: 0.06617205\n",
      "Epoch 525/100000 - Train Loss: 0.06603678\n",
      "Epoch 526/100000 - Train Loss: 0.06588683\n",
      "Epoch 527/100000 - Train Loss: 0.06578606\n",
      "Epoch 528/100000 - Train Loss: 0.06565825\n",
      "Epoch 529/100000 - Train Loss: 0.06545179\n",
      "Epoch 530/100000 - Train Loss: 0.06535768\n",
      "Epoch 531/100000 - Train Loss: 0.06520238\n",
      "Epoch 532/100000 - Train Loss: 0.06507214\n",
      "Epoch 533/100000 - Train Loss: 0.06495300\n",
      "Epoch 534/100000 - Train Loss: 0.06483726\n",
      "Epoch 535/100000 - Train Loss: 0.06466276\n",
      "Epoch 536/100000 - Train Loss: 0.06456147\n",
      "Epoch 537/100000 - Train Loss: 0.06443989\n",
      "Epoch 538/100000 - Train Loss: 0.06431021\n",
      "Epoch 539/100000 - Train Loss: 0.06416736\n",
      "Epoch 540/100000 - Train Loss: 0.06399935\n",
      "Epoch 541/100000 - Train Loss: 0.06389561\n",
      "Epoch 542/100000 - Train Loss: 0.06377054\n",
      "Epoch 543/100000 - Train Loss: 0.06363987\n",
      "Epoch 544/100000 - Train Loss: 0.06352026\n",
      "Epoch 545/100000 - Train Loss: 0.06337409\n",
      "Epoch 546/100000 - Train Loss: 0.06324959\n",
      "Epoch 547/100000 - Train Loss: 0.06311748\n",
      "Epoch 548/100000 - Train Loss: 0.06300449\n",
      "Epoch 549/100000 - Train Loss: 0.06286297\n",
      "Epoch 550/100000 - Train Loss: 0.06269550\n",
      "Epoch 551/100000 - Train Loss: 0.06261229\n",
      "Epoch 552/100000 - Train Loss: 0.06249686\n",
      "Epoch 553/100000 - Train Loss: 0.06232799\n",
      "Epoch 554/100000 - Train Loss: 0.06224683\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train = tc.tensor(np.asarray(X_train), dtype=tc.float32)\n",
    "X_test = tc.tensor(np.asarray(X_test), dtype=tc.float32)\n",
    "y_train_oh = tc.tensor(np.asarray(y_train_oh), dtype=tc.float32)\n",
    "y_test = tc.tensor(np.asarray(y_test), dtype=tc.int64)\n",
    "\n",
    "print(X_train.get_device())\n",
    "\n",
    "# Inisialisasi model custom FFNN\n",
    "custom_model = FFNN(\n",
    "    [784, 256, 128, 64, 10],\n",
    "    activations_list=['relu', 'relu', 'relu', 'softmax'],\n",
    "    loss_function='cce',  \n",
    "    weight_init='he_xavier',\n",
    "    init_params={'lower': -1, 'upper': 1}\n",
    ")\n",
    "epochs_custom = 100000\n",
    "batch_size_custom = 200\n",
    "lr_custom = 0.001\n",
    "\n",
    "start_time = time.time()\n",
    "history_custom = custom_model.train(X_train, y_train_oh, epochs=epochs_custom, batch_size=batch_size_custom, learning_rate=lr_custom, verbose=True)\n",
    "custom_training_time = time.time() - start_time\n",
    "print(f\"Model custom selesai training dalam {custom_training_time:.2f} detik.\")\n",
    "\n",
    "# Prediksi \n",
    "y_pred_custom = custom_model.forward(X_test)\n",
    "y_pred_custom_labels = tc.argmax(y_pred_custom.data, dim=1)\n",
    "accuracy_custom = (y_pred_custom_labels == y_test).float().mean().item()\n",
    "print(\"Akurasi model custom FFNN:\", accuracy_custom)\n",
    "\n",
    "\n",
    "#107,32s tc cpu\n",
    "#158,16s tc gpu\n",
    "#215,52s cp gpu\n",
    "#230,48s np cpu\n",
    "\n",
    "#555.06 detik\n",
    "\n",
    "#4000 20m\n",
    "\n",
    "# Model custom selesai training dalam 1135.40 detik. GPU\n",
    "# Akurasi model custom FFNN: 0.9748571515083313\n",
    "\n",
    "# Model custom selesai training dalam 1193.06 detik. CPU\n",
    "# Akurasi model custom FFNN: 0.9740714430809021\n",
    "\n",
    "# Model custom selesai training dalam 733.88 detik.\n",
    "# Akurasi model custom FFNN: 0.9735714197158813\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
